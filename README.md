# SystÃ¨me RAG (Retrieval-Augmented Generation)

![Version](https://img.shields.io/badge/version-1.0.0-blue.svg)
![Python](https://img.shields.io/badge/Python-3.10+-green.svg)

Un systÃ¨me de Retrieval-Augmented Generation (RAG) complet qui amÃ©liore les modÃ¨les de langage en les connectant Ã  des sources de donnÃ©es externes. Ce projet combine une indexation vectorielle avancÃ©e avec un LLM local ou l'API OpenAI pour fournir des rÃ©ponses prÃ©cises et contextuelles.

## âœ¨ FonctionnalitÃ©s

- ğŸ“„ **Gestion intelligente des documents** - Supporte les formats TXT et PDF avec chunking optimisÃ©
- ğŸ” **Recherche sÃ©mantique** - Indexation vectorielle avec TF-IDF ou Sentence Transformers
- ğŸ§  **ModÃ¨les de langage flexibles** - Fonctionne avec des modÃ¨les GGUF locaux (TinyLlama, Llama, Mistral) ou l'API OpenAI
- ğŸ”„ **API REST complÃ¨te** - Endpoints pour toutes les fonctionnalitÃ©s via FastAPI
- ğŸ–¥ï¸ **Interface utilisateur web** - Interface intuitive pour interagir avec le systÃ¨me

## ğŸ—ï¸ Architecture

Le systÃ¨me est construit avec une architecture modulaire :

```
RAGSystem/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ static/              # Fichiers statiques (CSS, JS)
â”‚   â”œâ”€â”€ templates/           # Templates HTML
â”‚   â”œâ”€â”€ document_processor/  # Traitement des documents
â”‚   â”œâ”€â”€ vector_store/        # Indexation et recherche vectorielle
â”‚   â”œâ”€â”€ llm/                 # Interface avec les modÃ¨les de langage
â”‚   â”œâ”€â”€ rag_pipeline/        # Pipeline RAG complet
â”‚   â”œâ”€â”€ api/                 # Endpoints API
â”‚   â””â”€â”€ main.py              # Point d'entrÃ©e principal
â”œâ”€â”€ data/                    # DonnÃ©es et index
â”œâ”€â”€ models/                  # ModÃ¨les de langage
â”œâ”€â”€ logs/                    # Fichiers de log
â””â”€â”€ run_api.py               # Script de dÃ©marrage
```

## ğŸ“¦ Fichiers volumineux non inclus

âš ï¸ **Attention** : Certains fichiers volumineux, notamment les modÃ¨les comme `models/tinyllama.gguf`, **ne sont pas inclus dans ce dÃ©pÃ´t** car ils dÃ©passent la limite de taille imposÃ©e par GitHub (100 Mo par fichier).

Si vous souhaitez utiliser ce projet, veuillez :

1. TÃ©lÃ©charger le fichier `tinyllama.gguf` depuis [Hugging Face](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/blob/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf)
2. Le placer manuellement dans le rÃ©pertoire suivant du projet :
   ```
   models/tinyllama.gguf
   ```

â— Assurez-vous que ce fichier est prÃ©sent pour que le systÃ¨me fonctionne correctement avec des modÃ¨les locaux.

## ğŸš€ Installation

### PrÃ©requis

- Python 3.10 ou supÃ©rieur
- Pip (gestionnaire de paquets Python)
- 4 Go RAM minimum (8 Go recommandÃ©)

### Ã‰tapes d'installation

1. **Cloner le dÃ©pÃ´t**
   ```bash
   git clone https://github.com/votre-nom/RAGSystem.git
   cd RAGSystem
   ```

2. **Installer les dÃ©pendances**
   ```bash
   python install_dependencies.py
   ```

3. **Configuration pour l'API OpenAI (Optionnel)**
   
   CrÃ©ez un fichier `.env` Ã  la racine du projet :
   ```
   OPENAI_API_KEY=votre-clÃ©-api
   ```

4. **TÃ©lÃ©charger un modÃ¨le LLM local (pour l'utilisation sans OpenAI)**
   ```bash
   python download_model.py
   ```

## ğŸ’» Utilisation

### DÃ©marrer le serveur

```bash
python run_api.py
```

AccÃ©dez Ã  l'interface via votre navigateur Ã  l'adresse : http://127.0.0.1:8000

### Utilisation du systÃ¨me RAG

1. **TÃ©lÃ©charger des documents** : Utilisez l'interface web pour ajouter des documents Ã  votre base de connaissances.
2. **Poser des questions** : Entrez vos questions dans la zone de chat pour obtenir des rÃ©ponses basÃ©es sur vos documents.
3. **Explorer les sources** : Chaque rÃ©ponse affiche les sources utilisÃ©es pour la gÃ©nÃ©ration, avec des scores de pertinence.

## ğŸ” CaractÃ©ristiques dÃ©taillÃ©es

### Module Document Processor

- Chargement et traitement de fichiers TXT et PDF
- DÃ©coupage intelligent en chunks avec chevauchement
- Gestion des mÃ©tadonnÃ©es et organisation hiÃ©rarchique

### Module Vector Store

- Support pour TF-IDF et Sentence Transformers
- Indexation vectorielle optimisÃ©e
- Recherche par similaritÃ© cosinus avec seuil de confiance

### Module LLM

- Support pour les modÃ¨les GGUF (Llama, TinyLlama, Mistral, etc.)
- IntÃ©gration avec l'API OpenAI
- GÃ©nÃ©ration de prompts optimisÃ©s selon le type de modÃ¨le

## ğŸŒ Technologies utilisÃ©es

- **FastAPI** : Framework API web rapide et moderne
- **scikit-learn / FAISS** : BibliothÃ¨ques d'indexation vectorielle
- **ctransformers** : Interface pour les modÃ¨les de langage locaux
- **OpenAI API** : Pour l'accÃ¨s Ã  des modÃ¨les de langage avancÃ©s
- **Bootstrap** : Framework CSS pour l'interface utilisateur

## ğŸ“œ Utilisation et Attribution

Ce code est libre d'utilisation. Toutefois, si vous utilisez ou adaptez ce projet, merci de mentionner l'auteur original:

**DÃ©veloppÃ© par Omar Ouchchen**

## ğŸ‘¥ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  ouvrir une issue ou Ã  soumettre une pull request pour amÃ©liorer ce projet.

## ğŸ“ Contact

Pour toute question ou suggestion, veuillez me contacter Ã  [ouchcheno@gmail.com](mailto:ouchcheno@gmail.com).
